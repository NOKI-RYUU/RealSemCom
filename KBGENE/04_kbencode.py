import os
import torch
import numpy as np
import json
import timm
import shutil
from torchvision import transforms
from torchvision.datasets import ImageFolder
from tqdm import tqdm
from datetime import datetime


class ImageEncoder:
    def __init__(self, dataset_path, output_root, model_name="vit_base_patch16_224", device="cuda"):
        """
        åˆå§‹åŒ–å›¾åƒç¼–ç å™¨
        :param dataset_path: æ•°æ®é›†è·¯å¾„ï¼Œæ¯ä¸ªç±»åˆ«æ˜¯ä¸€ä¸ªå­æ–‡ä»¶å¤¹
        :param output_root: å­˜æ”¾å·²ç¼–ç ç‰¹å¾çš„è·¯å¾„
        :param model_name: ViT æ¨¡å‹
        :param device: è®¾å¤‡ (cuda/cpu)
        """
        self.device = device
        self.dataset_path = dataset_path
        self.output_path = os.path.join(output_root, "encoded_features")
        os.makedirs(self.output_path, exist_ok=True)

        self.model = timm.create_model(model_name, pretrained=True, num_classes=0).to(self.device).eval()
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])

        self.metadata = []  # å­˜å‚¨å›¾ç‰‡è·¯å¾„å’Œç±»åˆ«ä¿¡æ¯
        self.features = []  # å­˜å‚¨ç‰¹å¾å‘é‡

        self.log_file = os.path.join(self.output_path, "encoding_log.txt")
        with open(self.log_file, "w") as log:
            log.write(f"ç¼–ç ä»»åŠ¡å¼€å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            log.write(f"æ•°æ®é›†è·¯å¾„: {self.dataset_path}\n\n")

    def log(self, message):
        """æ‰“å°å¹¶è®°å½•æ—¥å¿—"""
        print(message)
        with open(self.log_file, "a") as log:
            log.write(message + "\n")

    def encode_image(self, image):
        """ä½¿ç”¨ ViT æå–å›¾åƒç‰¹å¾"""
        image = image.unsqueeze(0).to(self.device)
        with torch.no_grad():
            feature = self.model(image)
        return feature.cpu().numpy()

    def process_dataset(self):
        """éå†æ•°æ®é›†å¹¶ç¼–ç æ‰€æœ‰å›¾åƒ"""
        dataset = ImageFolder(root=self.dataset_path, transform=self.transform)
        self.log(f"å‘ç° {len(dataset.imgs)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ç¼–ç ...")

        for img_path, class_idx in tqdm(dataset.imgs, desc="ğŸ” ç¼–ç å›¾åƒ", unit="img"):
            image = dataset.loader(img_path)  # è¯»å–å›¾åƒ
            image = self.transform(image)  # é¢„å¤„ç†
            feature = self.encode_image(image)  # æå–ç‰¹å¾

            # å­˜å‚¨æ•°æ®
            self.features.append(feature.flatten())
            self.metadata.append({
                "image_path": img_path,
                "category": dataset.classes[class_idx]  # è®°å½•ç±»åˆ«åç§°
            })

        # ä¿å­˜ç‰¹å¾å‘é‡å’Œå…ƒæ•°æ®
        np.save(os.path.join(self.output_path, "image_features.npy"), np.array(self.features))
        with open(os.path.join(self.output_path, "image_metadata.json"), "w") as f:
            json.dump(self.metadata, f, indent=4)

        self.log(f"âœ… ç¼–ç å®Œæˆï¼å·²ä¿å­˜ {len(self.features)} æ¡æ•°æ®ã€‚")


if __name__ == "__main__":
    dataset_path = "./data/images"  # æ•°æ®é›†è·¯å¾„
    output_root = "./knowledge_bases_coco_crop"  # å­˜æ”¾ç¼–ç åçš„æ•°æ®
    encoder = ImageEncoder(dataset_path, output_root, model_name="vit_base_patch16_224", device="cuda")
    encoder.process_dataset()
